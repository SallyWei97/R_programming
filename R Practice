# 70011 week2 -----------------------------------------------------------------------------------------------------------------
# compulsary exercise
summary(shipman)
summary(shipman$age)
describe(shipman$age)
table(shipman$decision)
prop.table(table(shipman$decision))
fre(shipman$decision)
killed <- subset(shipman, decision %in% c("Conviction", "Unlawful killing"))
killed
describe(killed$age)
hist(killed$age)
hist(killed$age,xlab = "Age",main = "Age Distribution of Patients Killed by Dr. Shipman")
hist(killed$year.died, xlab = "Year",
     main = "Year Distribution of Patients Killed by Dr. Shipman")
killed$name
p_load(devtools)
install.packages("devtools")
devtools::install_github("kalimu/genderizeR")
pacman::p_load(genderizeR)
killed$first.name <- sub(" .*", "", killed$name)
gender <- genderize(killed$first.name,genderDB=findGivenNames(killed$first.name))

# optional exercise
library(readr)
turnout <- read_csv("R studio/turnout.csv")
head(turnout)
summary(turnout)
summary(turnout$total)
table(turnout$year)
turnout$VAP.tr <- turnout$total / (turnout$VAP + turnout$overseas)
summary(turnout$VAP.tr) # VAP投票率
turnout$VAP.tr <- turnout$total /
  (turnout$VAP + turnout$overseas) * 100 # 百分比投票率
summary(turnout$VAP.tr)
turnout$VAP.diff <- turnout$ANES - turnout$VAP.tr # Q3.VAP.tr和ANES的投票率差异
summary(turnout$VAP.diff) 
turnout.sub1 <- subset(turnout, year == 1980| year == 1984| year == 1988|year == 1992|year == 1996|year == 2000|year == 2004|year == 2008)
turnout.sub11 <- subset(turnout, year == 1980| year == 1984| year == 1988|year == 1992|year == 1996|year == 2000|year == 2004)
# 提取选举年的数据
turnout.sub2 <- subset(turnout, year == 1982| year == 1986| year == 1990|year == 1994|year == 1998|year == 2002)
turnout.sub22 <- subset(turnout, year == 1982| year == 1986| year == 1990|year == 1994|year == 1998)
# 提取中间年的数据

# 遗留问题：按条件提取的条件能否是向量？

summary(turnout.sub1$ANES)
summary(turnout.sub11$ANES)
summary(turnout.sub2$ANES)
summary(turnout.sub22$ANES)
# Q4. 分析whether the self-reported bias from the ANES varies across election types
# results：yes，bias of presidential elections is smaller

turnout$VEP.tr <- turnout$total / (turnout$VEP + turnout$overseas) *100 # VEP投票率
turnout$ANES.diff <- turnout$ANES - turnout$VEP.tr
turnout.former <- subset(turnout, year ==1980 |year ==1982 |year ==1984 |year ==1986 |year ==1988 |year ==1990 |year ==1992)
turnout.later <- subset(turnout, year == 1994 |year ==1996 |year ==1998 |year ==2000 |year ==2002 |year ==2004 |year ==2008)
summary(turnout.former$ANES.diff)
summary(turnout.later$ANES.diff)
# Q5. Now has the bias of the ANES increased over time?
# results: yes

turnout$VAP.ad <- (turnout$total - turnout$osvoters) / (turnout$VAP - turnout$felons - turnout$noncit) *100
turnout$VAP.ad[14];turnout$VAP.tr[14];turnout$VEP.tr[14];turnout$ANES[14]
# Q6.  Compare the adjusted VAP turnout with the unadjusted VAP, VEP, and the ANES turnout rate
# results: VAP.tr < VEP.tr < VAP.ad < ANES

# 70151 week3 -----------------------------------------------------------------------------------------------------------------
# Q4
# X~gamma(7,1)
# (1)
1-pgamma(5, shape = 7, rate = 1)
# (2)
pgamma(15, shape = 7, rate = 1)-pgamma(10, shape = 7, rate = 1)


# Example
library(ggplot2)
set.seed(34643) # 固定生成的随机数
x<- 0:10
fx <- dpois(x,2) # 泊松分布密度
mass<-ggplot()+ geom_bar(mapping=aes(x=x, y=fx), stat="identity") + xlim(0,10) + ylim(0,0.4)
# geom_bar 绘制柱状图；aes 映射函数；xlim/ylim xy轴长度
mass

# Exercise1
set.seed(34643) # 固定生成的随机数
x<- 0:10
Fx <- ppois(x,2) # 泊松分布累积密度
mass<-ggplot()+ geom_bar(mapping=aes(x=x, y=Fx), stat="identity") + xlim(0,10) + ylim(0,1)
mass

set.seed(34643)
rbinom(1,1,0.5)
rbinom(100,1,0.5)

# Exercise2
set.seed(34643)
edf1 <- rpois(100,2) # 产生泊松分布的随机数
set.seed(34643)
edf2 <- rpois(100000,2)

mass1 <- ggplot()+ geom_bar(aes(x = edf1,y = (..count..)/sum(..count..)))+ xlim(0,10) + ylim(0,0.4) # or stat(count)/sum(count)
mass1

mass2 <- ggplot()+ geom_bar(aes(x = edf2,y = stat(count)/sum(count)))+ xlim(0,10) + ylim(0,0.4)
mass2

# 70011 week3 -----------------------------------------------------------------------------------------------------------------
pacman::p_load(haven, expss, psych, sjPlot, tidyverse)
load("~/R studio/70011/IAT.RData")
glimpse(iat)
view_df(iat) # 保存（或显示）导入的SPSS、SAS或Stata数据文件的内容，或任何类似的标记的data.frame，以HTML表格形式展现。
fre(iat$year)
round(prop.table(table(iat$year)),2) # 提取year并计算百分比，取两位小数
hist(iat$year)
describe(iat$dscore)
# Q1
dim(iat) # 查看有多少参与者在2004年和2019年之间完成了IAT？
names(iat) # 查看这个数据框包含哪些变量
glimpse(iat) # 查看变量名称,数量等
view_df(iat) # 保存（或显示）导入的SPSS、SAS或Stata数据文件的内容，或任何类似的标记的data.frame，以HTML表格形式展现。
fre(iat$year)
hist(iat$year)

# Q2
describe(iat$dscore) # dscore的平均数

# Q3
# 密度图1
fig1 <- ggplot(iat) + # Specify the data
  geom_density(aes(dscore) ) # Specify the type of figure and variable to plot
# geom_density: y轴显示的是 "核心密度"，或某一特定点的概率
fig1
# 密度图2
fig1b <- ggplot(iat) +
  geom_density(aes(dscore), color = "darkblue", fill = "lightblue") + # Add colour
  geom_vline(aes(xintercept = mean(dscore)), # Add a vertical line for the y
             colour = "black", # Make the line black
             linetype = "dashed") + # And dashed
  labs(title = "D-Score Distribution", x = "D-Score", y = "Density") # Add labels
fig1b

# Q4
iat.ts <- iat %>% # 将数据传给下面的函数
  group_by(year) %>%  # 按year分类
  summarise(mean=mean(dscore)) # 确定取的平均数 %>%是什么意思？
iat.ts
fig2 <- ggplot(iat.ts) + # We're using the 'iat.ts' object, not the full 'iat' data frame
  geom_line(aes(x = year, y = mean) )
fig2
fig2b <- ggplot(iat.ts) + # Using the 'iat.ts' object
  geom_line(aes(x = year, y = mean), color = "darkblue" ) + # Add colour to the line
  scale_y_continuous(limits = c(0, .4) ) + # Change the endpoints of the y-axis
  labs(title = "Mean D-Score by Year", x = "Year", y = "D-Score") + # Add labels
  theme_bw() # Change the theme to 'Black and White'
fig2b # Print the figure

# Q5
## Create the SDO scale
## List the items that comprise the SDO scale
sdo.keys <- list(sdo = cs(jsdo1, jsdo2, jsdo3, jsdo4, jsdo5, jsdo6,
                          jsdo7, -jsdo8, -jsdo9, -jsdo10, -jsdo11, -jsdo12) ) # cs()将文字列表转换为字符向量
## Now create the SDO scale using the items identified above
sdo.test <- scoreItems(sdo.keys, iat, min = 1, max = 6) # scoreItems:找出每个人和每个量表的总分或平均分
## Check the newly created scales and retrieve the scale alpha
## Add the new SDO variable to the 'iat' data
iat$sdo <- sdo.test$scores
## [rescale01] Function to rescale a variable from 0 to 1
rescale01 <- function(x, ...) {(x - min(x, ...)) / ((max(x, ...)) - min(x, ...))} # （x-min）/(max-min)
## Rescale SDO and store this as a new variable called 'sdo01'
iat$sdo01 <- rescale01(iat$sdo, na.rm = TRUE)
describe(iat$sdo01) # Check the summary statistics for SDO
## Check the correlation between SDO and D-score
cor(iat$sdo01, iat$dscore, use = 'complete.obs')

# 70151 week4 ----------------------------------------------------------------------------------------
# Q1 geometric distribution
dgeom(0,0.6)+dgeom(1,0.6)+dgeom(2,0.6)
dgeom(3,0.6)+dgeom(4,0.6)

# Q2 Normal distribution
pnorm(q = 0.5)
pnorm(0.75)-pnorm(-0.25)
1-(pnorm(0.5)-pnorm(-0.5))

# Q3 Normal distribution
qnorm(0.75)
qnorm(0.05)
qnorm(0.65)
qnorm(0.25)

# Q4 Binomial distribution
3*0.5
3*0.5*(1-0.5)

# Q5 Gamma distribution
3/2
3/(2*2)
qgamma(p = 0.75,shape = 3,rate = 2)

# Q6 
Ex <- (-2)*0.1+(-1)*0.2+0*0.2+1*0.15+2*0.15+3*0.2
Ex
Vx <- (4*0.1+1*0.2+0*0.2+1*0.15+4*0.15+9*0.2)-(Ex*Ex)
Vx
3*Ex+2
(4^2)*Vx

# optional exercise
# example：gamma distribution
# 1 模拟伽马分布再计算频率
draws <- rgamma(10000, shape=2, rate=2)
under3 <- draws <= 3 # 判断draws中各个数是否<=3，under3是一个逻辑值
mean(under3)
# 2 计算伽马分布的概率
pgamma(3, shape=2, rate=2)
# 3 计算模拟数的期望和方差
mean(draws)
var(draws)

# practice1 Binomial
bin <- rbinom(10000,size = 10,prob = 0.4) #模拟
under5 <- bin <= 5
mean(under5) #模拟频率
pbinom(5,10,0.4) #概率
mean(bin)
10*0.4
var(bin)
10*0.4*0.6

# practice2 geometric
geo <- rgeom(n = 10000,prob = 0.4)
under2 <- geo <= 2
mean(under2)
pgeom(q = 2,prob = 0.4)
mean(geo) # 相差较大
1/0.4
var(geo)
0.6/(0.4*0.4)

# practice3 Poisson
poi <- rpois(100000,0.6)
under4 <- poi <= 4
mean(under4)
ppois(4,0.6)
mean(poi)
var(poi)

# practice4 Uniform
uni <- runif(n = 10000,min = 1,max = 5)
below2 <- uni <= 2
mean(below2)
punif(2,min = 1,max = 5)
mean(uni)
(1+5)/2
var(uni)
(5-1)^2/12

# practice5 Exponential
exp <- rexp(10000,rate = 0.3)
below3 <- exp <= 3
mean(below3)
pexp(3,0.3)
mean(exp)
1/0.3
var(exp)
1/(0.3*0.3)

# practice6 Gamma
gam <- rgamma(10000,shape = 2,rate = 7)
below4 <- gam <= 0.4
mean(below4)
pgamma(0.4,shape = 2,rate = 7)
mean(gam)
2/7
var(gam)
2/(7*7)

# practice7 Normal
nor <- rnorm(10000)
below1 <- nor <= 1
mean(below1)
pnorm(1)
mean(nor)
var(nor)

# 70011 week4 -------------------------------------------------------------------------------------------------------
## practical
```{r}
library('tidyverse')
library('expss')
library('haven')
library('psych')
```

1.
```{r}
chechen<- read.csv('chechen.csv')
head(chechen)
table(chechen$fire)
fre(chechen$fire)
```

2.
```{r}
chechen.sub<-subset(chechen,groznyy==1)
chechen.sub
summary(chechen.sub$deaths)
#summary(chechen$deaths[chechen$groznyy == 1]) #equivalent to the above
#aggregate(deaths ~ groznyy, data = chechen, mean)  #using 'groznyy' variable to group 'deaths' variable,and finding mean
#aggregate(deaths ~ groznyy, data = chechen, median)  #using 'groznyy' variable to group 'deaths' variable, and finding median
chechen %>% group_by(groznyy) %>% 
  summarise(mean = mean(deaths, na.rm = TRUE), 
            median = median(deaths, na.rm = TRUE)) #  #using 'groznyy' variable to group 'deaths' variable, and dinginf mean $ median
```

3.
```{r}
bombed1<-subset(chechen,fire==1)
nonbombed1<-subset(chechen,fire==0)
mean(bombed1$postattack,na.rm = TRUE)-mean(nonbombed1$postattack,na.rm = TRUE)
```

4.
```{r}
bombed2<-subset(chechen,fire==1)
nonbombed2<-subset(chechen,fire==0)
mean(bombed2$preattack)-mean(nonbombed2$preattack)
```

5.
```{r}
chechen<-chechen%>% mutate(diffattack=chechen$postattack-chechen$preattack)
bombed_diff<-subset(chechen,fire == 1)
mean(bombed_diff$diffattack)
```

6.
```{r}
unbombed_diff<-subset(chechen,fire == 0)
mean(bombed_diff$diffattack)-mean(unbombed_diff$diffattack)
```

## optional
---
title: "week 4 optional practice"
author: "zheng wei"
date: "2021/10/23"
output: pdf_document
---

```{r}
install.packages("plyr")
library(plyr)
install.packages("dplyr")
library(dplyr)
```

1.1 success正则
```{r}
grepl("^dies",leaders$result)
leaders$success <- ifelse(grepl("^dies",leaders$result) == "TRUE",1,0)
```

1.2 case_when
```{r}
install.packages("dplyr")
library(dplyr)
leaders <- leaders %>% mutate(success2= case_when(result=="not wounded"~0,
                                              result=="dies within a day after the attack"~1,
                                              result== "survives, whether wounded unknown"~0,
                                              result=="wounded lightly" ~0,
                                              result=="plot stopped"~0,
                                              result=="hospitalization but no permanent disability"~0,
                                              result=="dies between a day and a week"~1,
                                              result=="dies, timing unknown" ~1,
                                              result=="survives but wounded severely"~0,
                                              result=="dies between a week and a month"  ~1))

```

2. polity z分数
```{r}
leaders$polity.pre <- (leaders$politybefore - min(leaders$politybefore)) / (max(leaders$politybefore) - min(leaders$politybefore))
leaders$polity.pre
leaders$polity.post <- (leaders$polityafter - min(leaders$polityafter)) / (max(leaders$polityafter) - min(leaders$polityafter))
leaders$polity.post
```

3. difference in polity.post
```{r}
leaders %>% summarise(y = 
                mean(polity.post[success==1])
                -  mean(polity.post[success==0]))
```
4. 
```{r}
leaders %>% summarise(y = 
                mean(polity.post[success==1])
                -  mean(polity.pre[success==1]))
```
5. 
```{r}
leaders$polity.diff <- leaders$polity.post - leaders$polity.pre
leaders %>% 
  summarise(y = 
              mean(polity.diff[success==1])
                -  mean(polity.diff[success==0]))
```
6. 
```{r}
leaders$warafter <- 
case_when(leaders$civilwarafter == 1 ~ 1,
          leaders$interwarafter == 1 ~ 1,
          TRUE ~ 0)
leaders$warbefore <- 
case_when(leaders$civilwarbefore == 1 ~ 1,
          leaders$interwarbefore == 1 ~ 1,
          TRUE ~ 0)
leaders$war.diff <- leaders$warafter - leaders$warbefore
leaders %>% 
  summarise(y= mean(war.diff[success == 1]) - mean(war.diff[success == 0]))
leaders %>% 
  summarise(y= mean(war.diff[success == 1]))
leaders %>% 
  summarise(y= mean(war.diff[success == 0]))
```
# 70011 week 5 --------------------------------------------------------------------------------------------------
# pratcical lab
---
title: "lab05"
author: "zheng wei"
date: "2021/10/28"
output: html_document
---
0. install packages
```{r}
pacman::p_load(textstem, dplyr, textreadr, tidytext, tidyverse,
wordcloud)
pacman::p_load(ggplot2, httr, PKI, RColorBrewer, rmarkdown,
ROAuth, SentimentAnalysis, SnowballC, tm, twitteR, wordcloud)
```
1. capture/read/save
```{r}
df <-
'https://www.washingtonpost.com/wp-srv/national/longterm/unabomber/manifesto.text.htm' %>%
download() %>% # download data
read_document() # read data
save(df,file = "manifesto.txt") # cannot use 'write' to save character object.
head(df) 
tail(df, n = 10)
```
2. Data processing
```{r}
df <- df[-1:-5]
head(df, n = 3)
df <- df[-315:-321]
tail(df, n = 3) # delete the columns
df <- iconv(df, 'utf-8', 'ascii', sub = '') # change the form from utf-8 to ascii, change characters who cannot be identified into '' 
head(df, n = 1) 
tail(df, n = 1)
df <- gsub("\\n", "", df) # delete the'\n'
tail(df)
text_df <- tibble(text = df) # tidy the data(a new data frame)
text_df <-
  text_df %>%
    unnest_tokens(word, text) %>% # split the text into word
      anti_join(stop_words) # get rid of stop words
numbers <- text_df %>%
  filter(str_detect(word, "[0-9]")) %>% 
  # str_detect: estimate if an element exists(T/F)
  # "[0-9]"--Regular expression：select all the word which has 0-9
    select(word) %>% # select according to the estimate results
      unique() # remove duplicates
text_df <- text_df %>%
  anti_join(numbers, by = "word")  #get rid of 'numbers'
```
3. Data analysis
```{r}
freq_words <- text_df %>%
  count(word, sort = TRUE) # count():need to specify a data frame so can count columns(there should set a colname)
ggplot(freq_words, aes(n/sum(n) ) ) + # aes(1,2) 1 means the axis, 1 means the yaxis
  geom_histogram(show.legend = FALSE, bins = 40) # bins: number of bins
freq_rank <- freq_words %>%
  mutate(rank = row_number(),'frequency' = n/sum( n ) ) # mutate: add a new variable into exist data set
  # row_number: rank functions(only rank according to 'numeric')
```
4. Data drawing(bar)
```{r}
freq_rank %>%
  filter(rank < 20) %>% # extract the data if their rank<20
    ggplot(aes(x = reorder(word, -rank), y = frequency ) ) + # reorder: sort the first variable in positive order according to the second variable
      geom_bar(stat = "identity") + # 'identity': the height of the bar indicates the value of the data data(y=?is essential)
        xlab("Words") +
          ylab("Count") +
            coord_flip() # Reversing the xy axis
```
5. Data drawing(cloud)
```{r}
freq_rank <- freq_rank %>%
  group_by(word) %>%
    mutate(count = sum(n) ) # maybe meaningless operation?
freq_rank %>%
  with(wordcloud(word, count,  
    min.freq = 25, # the min frequency of words in the words cloud
    max.words = 50, # the max number of words in the words cloud
    random.order = TRUE,
    colors = brewer.pal(8, "Blues") # use 'brewer.pal.info' to check the all color set
                )
      )
```
6. Data Lemmatizing词形还原
```{r}
lemma <- textstem::lemmatize_strings(freq_words$word,dictionary = lexicon::hash_lemmas) # Default Lemma Dictionary
lemma_dictionary <- make_lemma_dictionary(x, engine = 'hunspell') # Hunspell Lemma Dictionary
lemma <- lemmatize_strings(text_df$word, dictionary = lemma_dictionary) # cannot use freq_words, because it was counted
glimpse(lemma)
lemma <- as.data.frame(lemma)
colnames(lemma) <- "word" # for further count
```
7. remove stop_words/numbers(Prevent them from reappearing after Lemmatizing)
```{r}
data(stop_words) # load the data set of 'stop_words'
head(stop_words)
lemma <-
  lemma %>%
    subset(!lemma %in% stop_words$word) # extract words that do not contain stop_words
# use the function filter will report an error
numbers <- lemma %>%
  filter(str_detect(word, "[0-9]")) %>% 
    select(word) %>% 
      unique() 
lemma <- lemma %>%
  anti_join(numbers, by = "word") 
```
9. corpus processing(语料库)
```{r}
freq_words2 <- lemma %>%
  count(word, sort = TRUE) # count():need to specify a data frame so can count columns(there should set a colname)
freq_rank2 <- freq_words2 %>%
  mutate(rank = row_number(),
    'frequency' = n/sum( n ) )
freq_rank2
```
10. corpus word cloud
```{r}
freq_rank2 %>%
  with(wordcloud(word, n,  
    min.freq = 25, # the min frequency of words in the words cloud
    max.words = 50, # the max number of words in the words cloud
    random.order = TRUE,
    colors = brewer.pal(8, "PuBu") # use 'brewer.pal.info' to check the all color set
                )
      )
```

```{r}

```

```{r}

```

# lab 2.0
---
title: "week5 lab2.0"
author: "zheng wei"
date: "2021/11/2"
output: html_document
---
0. install packages
```{r}
pacman::p_load(janitor, SnowballC, textdata, textreadr,tidytext, tidyverse, topicmodels, wordcloud, dplyr)
```
0.1 similar process
```{r}
df <-
'https://www.washingtonpost.com/wp-srv/national/longterm/unabomber/manifesto.text.htm' %>%
download() %>% # download data
read_document() # read data
save(df,file = "manifesto.txt") # cannot use 'write' to save character object.
head(df) 
tail(df, n = 10)
df <- df[-1:-5]
head(df, n = 3)
df <- df[-315:-321]
tail(df, n = 3) # delete the columns
df <- iconv(df, 'utf-8', 'ascii', sub = '') # change the form from utf-8 to ascii, change characters who cannot be identified into '' 
head(df, n = 1) 
tail(df, n = 1)
df <- gsub("\n", "", df) # delete the'\n'
tail(df)
```

```{r}
text <- tibble(text = df) # tidy the data(a new data frame)
```
1. data cleaning
```{r}
text <-
  text %>%
    mutate(chapter = # mutate: add a new column
      cumsum(str_detect( # cumsum: counter T/F
      # str_detect: estimate if an element exists(T/F)
      text, "^[A-Z]{2,}") # :upper: = A-Z
          ) ) 
# in order to divide them in to chapters(which has 2 upper)
```
```{r}
## Now ensure that each row has only 1 word
text_df <- # Create an object called text_df
  text %>% # Using this object
    unnest_tokens(word, text) %>% # split the text into word
      anti_join(stop_words) %>% # Merge without stop_words
        mutate(stem = wordStem(word)) # Stemming using 'SnowballC' package
```
1.1 similar process
```{r}
numbers <- 
  text_df %>%
  filter(str_detect(word, "[0-9]")) %>% 
  # str_detect: estimate if an element exists(T/F)
  # "[0-9]"--Regular expression：select all the word which has 0-9
    select(word) %>% # select according to the estimate results
      unique() # remove duplicates
text_df <- text_df %>%
  anti_join(numbers, by = "word")  #get rid of 'numbers'
freq_words <- text_df %>%
  count(word, sort = TRUE) # count():need to specify a data frame so can count columns(there should set a colname)
freq_words
```
2 rank stemmed words
```{r}
freq_stems <- text_df %>%
  count(stem, sort = TRUE)
freq_stems
```
3.1 similar drawing
```{r}
ggplot(freq_words, aes(n/sum(n) ) ) + # Plot each word / total words
  geom_density(show.legend = FALSE,
    fill = "lightblue")
```
3 drawing
```{r}
freq_stems <- freq_stems %>%
  mutate(rank = row_number(), # mutate adds columns 'rank' & 'rate'
    rate = n/sum( n ) * 100 )
freq_stems %>%
  filter(rank <= 20) %>% # Show top 20 terms (can change if desired)
    ggplot(aes(x = reorder(stem, -rank), y = rate ) ) +
      geom_bar(stat = "identity") +
      xlab("Stemmed Words") +
      ylab("Rate (per 100 words)") +
      coord_flip()
```
4. word cloud
```{r}
freq_stems %>% # Use this data
  with(wordcloud(stem, n, # Apply function to 'word' with `n` counts
    min.freq = 25, # Min occurrence of a given term
    max.words = 50, # Max terms to display
    random.order = TRUE, # Don't sort based on word frequency
    colors = brewer.pal(8, "Dark2") # Change the colours
                )
       )
```
5. sentiment analysis
```{r}
freq_words <- inner_join(freq_words,get_sentiments("bing"))
freq_words <- inner_join(freq_words,get_sentiments("afinn"))
summary(freq_words$value)

## Weighted afinn sentiment (to account for differential use)
freq_words <-
  freq_words %>%
  mutate(rate = n/sum( n ) * 100 , # for weighting
  wt.value = (value * rate) )
summary(freq_words$wt.value)
```
6. Topic modeling
```{r}
## Add a frequency count of stems to the data
text_df <-
  text_df %>%
    group_by(stem) %>% # Group by stem
      mutate(n = n() ) %>% # n(): gives the current group size
        ungroup() # Restore the grouped data
## Document-term matrix
dtm <- text_df %>%
  cast_dtm(chapter, stem, n)
dtm
```
6.1 unsupervised machine learning
```{r}
tm.2 <- dtm %>%
  LDA(k = 2)
topics.2 <- tidy(tm.2)
top.terms.2 <- topics.2 %>%
  group_by(topic) %>% # Organise by topic
    slice_max(beta, n = 10) %>% # Find top 10 words
     ungroup() %>% # Put back to normal (not fundamental)
      arrange(topic, -beta) # Sort by topic, highest to lowest beta
```
```{r}
## Create the comparison figure
top.terms.2 %>%
  mutate(term = reorder_within(term, beta, topic)) %>% 
  # term=term_topic(因为不同的topic有相同的term)
    ggplot(aes(beta, term, fill = factor(topic))) + # Color filling according to the topic factor
      geom_col(show.legend = FALSE) + # Does not show which color represents which factor
      facet_wrap(~ topic, scales = "free") + # divide Into 2 pics according to topic
      #scales = "free"：The default scales will put the vertical axis coordinates together
      scale_y_reordered() # add reordered term to the plot
```
6.2 comparison between 2 topics
```{r}
phi.ratio.2 <- topics.2 %>%
  mutate(topic = paste0("topic", topic)) %>% # topic=topic+number
    pivot_wider(names_from = topic, values_from = beta) %>% # transfer topic into a column
      filter(topic1 > .001 | topic2 > .001) %>%
        mutate(log_ratio = log2(topic2 / topic1))
```
```{r}
phi.ratio.2 %>%
  top_n(20, abs(log_ratio)) %>% # subset the 20 with the largest absolute value
    ggplot(aes(y = fct_reorder(term, log_ratio), # rank the term according to the value of log_ratio
      x = log_ratio)) +
      geom_col() +
      labs(y = "", x = "log ratio of phi between topic 2 and topic 1 (base 2)")
```

# lab 3.0
---
title: "week5 lab3.0"
author: "zheng wei"
date: "2021/11/3"
output: html_document
---

1. The Gutenberg Project
```{r}
## Loading the gutenbergr package
pacman::p_load(gutenbergr, janitor, SnowballC, textdata, textreadr,tidytext, tidyverse, topicmodels, wordcloud, dplyr)
```
```{r}
## Download Vol 2 of "Works of E. A. Poe" (2148 is the Gutenberg book ID)
eapoe_works <- gutenberg_download(2148)
```
```{r}
## Keep only the lines that correspond to the first 12 stories
eapoe_works <- eapoe_works %>% slice(42:5438)
## The titles of the stories (as they appear in the volume)
titles <- c("THE PURLOINED LETTER",
"THE THOUSAND-AND-SECOND TALE OF SCHEHERAZADE",
"A DESCENT INTO THE MAELSTROM.",
"VON KEMPELEN AND HIS DISCOVERY",
"MESMERIC REVELATION",
"THE FACTS IN THE CASE OF M. VALDEMAR",
"THE BLACK CAT.",
"THE FALL OF THE HOUSE OF USHER",
"SILENCE—A FABLE",
"THE MASQUE OF THE RED DEATH.",
"THE CASK OF AMONTILLADO.",
"THE IMP OF THE PERVERSE")
```
```{r}
## Build the corpus as a tidy data frame
eapoe_df <-
  eapoe_works %>%
    mutate(story =
    ifelse(str_detect(text, # 判断text是否存在于后者
    paste(titles, collapse = "|")), # 为titles加上分隔符，成为一整个字符串
    text, NA)) %>% # 如果存在，则输出text该行的值(即titles)，否则输出NA
      fill(story) %>% # 自动填充story的NA值(填充为上一个非NA值)
        mutate(story = factor(story, levels = unique(story))) # 将story转为因子类型
```
2. Data cleaning
```{r}
# split into words
eapoe_df <- tibble(eapoe_df)
eapoe_df <- eapoe_df %>% 
  unnest_tokens(word,text) %>%
    anti_join(stop_words) %>%
      mutate(stem = wordStem(word))
```
```{r}
# delete numbers
numbers <- eapoe_df %>%
  filter(str_detect(word,"[0-9]")) %>%
    select(word) %>%
      unique()
eapoe_df <- eapoe_df %>%
  anti_join(numbers,by = "word")
```
```{r}
# rank words and stemmed words
freq_words <- eapoe_df %>%
  count(word,sort = TRUE)
freq_stems <- eapoe_df %>%
  count(stem, sort = TRUE)
```
2. frequence drawing
```{r}
freq_words %>% ggplot(aes(n/sum(n))) +
  geom_density(show.legend = FALSE,
    fill = "lightblue")
```
```{r}
freq_stems <- freq_stems %>%
  mutate(rate = 100 * n/sum(n), 
         rank = row_number())
freq_stems %>% 
  filter(rank <= 20) %>% 
      ggplot(aes(x = reorder(stem, -rank), y = rate)) +
      geom_bar(stat = "identity") +
      xlab("Stemmed Words") +
      ylab("Rate (per 100 words)") +
      coord_flip()
```
3. WORD CLOUD
```{r}
freq_stems %>% 
  with(wordcloud(stem,n,
        min.freq = 20,
        max.words = 50,
        random.order = TRUE,
        colors = brewer.pal(11,"Set3")))
```
4. Sentiment Analysis
```{r}
# get sentiment
freq_words <- inner_join(freq_words,get_sentiments("bing"))
freq_words <- inner_join(freq_words,get_sentiments("afinn"))
```
```{r}
# weighted
freq_words <- freq_words %>% 
  mutate(rate = 100* n/sum(n), 
         wt.value = value * rate)
summary(freq_words$wt.value)
```
5. Topic Modelling
```{r}
## Add a frequency count of stems to the data
eapoe_df <- eapoe_df %>% 
  group_by(story) %>% 
    mutate(n = n()) %>% 
      ungroup()
dtm <- eapoe_df %>% cast_dtm(story, stem, n)
# story:documents(like factor),n:value,stem:term.
```
```{r}
# LDA Modeling
lm.2 <- dtm %>% LDA(k=2)
```
```{r}
# data processing
topic2 <- tidy(lm.2)
topic2 <- topic2 %>% 
  group_by(topic) %>% 
    slice_max(beta, n = 10) %>% 
        arrange(topic, -beta)
```
```{r}
# drawing
topic2 %>% 
  mutate(term = reorder_within(term, by = beta, within = topic)) %>% 
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col() +
    facet_wrap(~ topic, scales = "free") +
    scale_y_reordered()
```

